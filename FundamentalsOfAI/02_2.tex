% vim:ft=tex:
%
\documentclass[UTF8]{ctexart}
\usepackage{amsmath}
\usepackage{listings}

\lstset{
    basicstyle          =   \sffamily,          % 基本代码风格
    keywordstyle        =   \bfseries,          % 关键字风格
    commentstyle        =   \rmfamily\itshape,  % 注释的风格，斜体
    stringstyle         =   \ttfamily,  % 字符串风格
    flexiblecolumns,                % 别问为什么，加上这个
    numbers             =   left,   % 行号的位置在左边
    showspaces          =   false,  % 是否显示空格，显示了有点乱，所以不现实了
    numberstyle         =   \zihao{-5}\ttfamily,    % 行号的样式，小五号，tt等宽字体
    showstringspaces    =   false,
    captionpos          =   t,      % 这段代码的名字所呈现的位置，t指的是top上面
    frame               =   lrtb,   % 显示边框
}

\lstdefinestyle{Python}{
    language        =   Python, % 语言选Python
    basicstyle      =   \zihao{-5}\ttfamily,
    numberstyle     =   \zihao{-5}\ttfamily,
    keywordstyle    =   \color{blue},
    keywordstyle    =   [2] \color{teal},
    stringstyle     =   \color{magenta},
    commentstyle    =   \color{red}\ttfamily,
    breaklines      =   true,   % 自动换行，建议不要写太长的行
    columns         =   fixed,  % 如果不加这一句，字间距就不固定，很丑，必须加
    basewidth       =   0.5em,
}
\title{
	数学基础
}
\author{
	Letian Lin --- \texttt{yingziyu-Lin@outlook.com}
}

\begin{document}
\maketitle
\section{概率论}
\subsection{概率的定义}
频率学派：一个事情发生的次数占总试验次数的比例

贝叶斯学派：概率是表达个人或主观信念的不确定性

一定是$[0,1]$上的一个数

\subsection{联合概率}
事件$X=x_i$和$Y=y_i$同时发生的概率，记作$P(X=x_i,Y=y_j) = \frac{n_{i,j}}{N}$

加和原则：$P(X=x_i)=\sum^L_{j=1}P(X=x_i,Y=Y_i)$

\subsection{条件概率}
在事件$X=x_i$发生条件下，事件$Y=y_j$发生的概率，记作$P(X=x_i|Y=y_j)=\frac{n_{i,j}}{c_i}$

$p(X,Y)=p(Y|X)*p(X)$

\subsection{贝叶斯定理}
$$P(A|B)=\frac{P(B|A)*P(A)}{P(B)}$$

$P(B|A)$称作似然度（表达了对a的不同设置，观测到数据集有多大的可能性），$P(A|B)$叫做后验概率（观察到B后的概率），P(A)为先验概率，是对a的猜测

b是观察到的数据，a是模型参数。

$$\textbf{posterior=likeihood*prior}$$

\paragraph{推导}
$$P(Y|X)*P(X)=P(Y,X)=P(X,Y)=P(X|Y)*P(Y)$$

\subsection{独立事件}
两个事件互不影响，Y事件不影响X事件$p(X|Y)=p(X),p),p(Y|X)=p(Y),p(X,Y)=p(X)p(Y)$

\subsection{概率密度/累计分布函数}
$p(x)$概率密度函数(PDF)

$P(x)$累计分布函数(CDF)

性质：$p(x)\geq 0$
$$P(z)=\int$$

\subsection{数学期望}
\paragraph{定义}在概率分布$p(x)$下$f(x)$的均值
\paragraph{计算}$E(f)=\sum_xp(x)f(x)$

$$E(f)=\int p(x)f(x)$$

$$E(f)=\frac{1}{N}\sum^N_j=1f(j)$$

\subsection{协方差}
一个变量偏离期望的时候，另一个变量也偏离期望的趋势

\subsection{随机变量的分布}
\paragraph{高斯分布}$$\mathcal {N} (x;\mu,\sigma^2)=\sqrt {\frac {1} {2\pi\sigma^2}}exp (-\frac {1} {2\sigma^2} (x-\mu)^2)$$

\subsection{学派}
\paragraph{频率学派}用可重复的事件来计量事情发生的可能性

\subsection{最大似然估计，最大后验估计}
$$\theta_{MLE}=arg\max{log p(X|\theta)},p(X|\theta)=\Pi^n_{i=0}p(x_i|\theta)$$

最大后验估计：将参数视为一个随机变量，从一个先验分布$p(\theta)$中取样而来。

$$\theta_{MAP}=arg\max\ p(\theta|X)=argmax\ p(X|\theta)p(\theta)$$

MLE和MAP的区别：MLE高度依赖于数据，相信数据是均匀采样的。而MAP依赖于先验模型，相信数据符合某种分布。

\section{信息论}
\subsection{直觉}不太发生的事情发生了比很可能发生的事情发生了更有信息量

概率越小的事情发生，信息量越大。

希望用一种方式对信息进行量化：可能性越小，信息量越大;必然事件没有信息量;独立事件的信息应该具有可加性

$$I(x)=-\log P(x)$$

香农熵：量化整个概率分布中的不确定性，香农熵越大，信息量越大。
$$H(x)=E_{x~P}[I(x)]$$

是该分布中抽取事件信息量的期望值，给出了平均编码从分布P中抽取符号所需的位数的下界。

KL散度：有两个关于同一个随机变量的两个不同分布P和Q,可以用KL散度衡量两个分布有多少不同。

$$D_{KL}(P||Q)=E[\log \frac{P(x)}{Q(x)}]=E[\log P(x) - \log Q(x)]$$

KL散度是非负的，它是不对称的。

交叉熵：和KL散度类似，但是多了一项香农熵。多数情况下
$$H(P,Q)=H(P) + D_{KL}(P||Q)$$
\end{document}
