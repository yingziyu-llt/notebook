给定分布函数𝑝(𝒙; 𝜽)，其中𝜽是未知参数，以及观测到的数据集𝒟，学习参数𝜽的过程可视为从数据集𝒟来估计𝜽的过程

有两种方法：视𝜽为未知变量，学习的目标是求解$𝜽^∗ = argmax_𝜽 p(D; 𝜽)$这种是概率学派的。另一种视𝜽为未知随机变量，学习的目标是求解后验分布$argmax_𝜽 p(D; 𝜽)$，这是贝叶斯方法。

## 最大似然估计
$$\theta_{MLE}=arg\max_\theta p(D;\theta)$$
也就是，找到让其likelyhood可能性最大的参数。
似然函数：$$p(D;\theta)=\prod_{n=1}^N p(x_n;\theta)$$一般用其对数形式即对数似然。为了统一为最小化，可以用负对数似然$$NLL(\theta)=-\sum_{n=1}^N\log(x_n;\theta)$$
对于有标签数据集，负对数似然有两种形式，但其优化目标是等价的。
$$\prod_{n=1}^N p(y_n|x_n;\theta) \ or \ \prod_{n=1}^N p(x_n,y_n;\theta)$$两个优化目标之间差了一个常数的倍数。

从KL散度的角度看，优化NLL实际上是优化$KL(\hat{p}_D || q)$，可以证明这两个是等价的。
![[Pasted image 20250928001128.png]]

最大似然估计的优化目标是尽量使得看到的数据出现的概率最大。这导致其对未见到的数据难于给出合适得概率估计，造成过拟合现象（overfitting，或称过学习）

## 最大后验估计
