= 对抗搜索

== 双人游戏 问题模型

相对于传统的搜索，多了一个Player(s)，用来决定是谁在玩游戏。

== Min-Max搜索

=== 基本想法

假设对手和你一样聪明，他总是最小化你的收益，你自己总是最大化自己的收益。极大极小值搜索就是在决策树上的游历，假设敌人和你一样聪明。

=== 算法流程

对每个节点展开，求其子节点的最大值（MAX节点）。

=== 时间复杂度分析
假设深度为m,合法操作为b，那么时间复杂度为$O(m^b)$。

=== 学习

==== 死记硬背和一般化学习
对每部的估值评分是从前面的MINMAX搜索中得出的；一般化是将估值函数变为可学习的，将估值函数和真实情况相拟合。

== alpha-beta剪枝

=== 想法

MIN-MAX搜索有大量无效搜索，用alpha-beta剪枝，减少无效搜索。

=== 算法流程

对于min节点$->$如果子节点有值比父节点现在求出的max要小，那么不继续展开，直接剪枝。
对于max节点$->$如果子节点有值比父节点现在求出的min要小，那么不继续展开，直接剪枝。

=== 总结
剪掉不影响解的节点，加快搜索速度，但对于大问题规模依然不能解决搜索树过大的情况。

== 不完美实时决策

=== 基本想法

对于过大的问题，不能搜完所有节点再做决定，所以可以使用启发式函数取代直接阶段搜索，将非叶子节点转化为叶子节点。

=== 搜索流程

对于alpha-beta剪枝，用cutoff test替换terminal test，用来决定什么时候调用局面估计；用可以估计状态的收益的启发式函数的值来代替utility函数。

对于设计局面估计，可以尝试当前局面赢的期望，可以表示为$"Eval"(s) = sum^n_"i=1" w_i f_i(s)$

要求：每个特征的贡献是彼此独立的。

可以用机器学习的方法学习出状态估值函数。

cutoff test: 用来决定是否继续搜索，一种是用一个确定的d设定为搜索深度，也可以进行迭代加深的方法。

对于终局，使用真实结果。

还可以用估值函数对子节点排序，从而加速alpha-bata剪枝。

== 蒙特卡洛树搜索

=== 蒙特卡洛方法

统计模拟方法。就是进行大量随机抽样，通过计算统计量，从而得到某种近似解的过程。

=== 基本想法

在决策空间中随机采样，根据结果构建搜索树在给定与中求解最优决策。
是一种基于统计的方法，更强的计算能力就可以带来更好的性能表现。

=== 随机蒙特卡洛方法
对n个儿子对敌我双方都纯随机的方法搜索到终局，对每个儿子搜索M次，求平均值作为儿子的胜率，选择胜率最高的作为下一步。

=== 优化
用贪心树搜索选择一个节点，然后最新选择的节点做模拟再根据结果更新搜索树。
选择->扩张->模拟->反传
