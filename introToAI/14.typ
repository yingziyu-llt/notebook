= 强化学习

== 基础 斯金纳箱

== 强化学习的模型

$"f(状态"s_i+"回报"R_i->"动作"A_i$

1. 初始状态$S_0$(state)
2. 动作$A$(action)
3. 当前玩家$C$(current player)
4. 状态转移$P$(transition) $P(S_t+1 | S_t,A_t)$，用所有可达的状态构成状态空间，所有状态下可行动作构成动作空间
5. 终止状态$S_T$(terminal state)
6. 奖励$R$(reward) $R_t <- S_t, A_t$

=== 智能体（问题的解）

- 策略$pi$：$A_t <- pi (S_t)$用来表示智能体
  - 给出了智能体在任何条件下如何选择A的方案
  - 策略应当是全局性的。

目标：

寻找最优的策略$pi$，使得从初始状态到终止状态的奖励最多$max sum^n_i R_i$

== 流程
=== 建立估值表
先设定初值，赢了的局面设为1,输了的局面设为0.其他按照设成0.5。

=== 和对手玩很多次
- 利用：大概率用贪心选择价值最大的地方取下
- 探索：偶尔随机选择

=== 边下边修
初始值：只有终局的价值是对的，所有中间局面的价值都是0.5

过程中：状态价值从后面向前传导。

分析：打出终局之后，反向传播，更新所有权值，假设一直在某路径上走，走着走着就可以知道某个节点向某处走是赢还是输

$V(S_t)<-V(S_t) + alpha (V(S_t+1)-V(S_t))$

== 扩展：非确定性

=== 状态转移$P$和奖励$R$

状态转移不一定是确定性的，用概率状态转移。

$P$状态转移函数$<S,A,S'>->RR^+$,$P(s,a,s') = P(S' | S,A)$,要求$sum_s' P(s,a,s') = 1$

奖励也是概率，和$P$类似。

=== 策略$pi$和累计收益$G$

策略也可以是有随机性，非确定的，是一个概率分布。

折扣因子$gamma$：未来收益的重要程度。$0<=gamma<=1$。0退化为贪心，1认为远端收益和现在的收益一样重要。

累计收益$G=R_1+gamma R_2+gamma^2 R_3+...=sum^n_i gamma^(i - 1) R_i$

=== 评估
评价从s出发执行策略$pi$的累计收益。
$v_pi = EE_pi [G_t|S_t = s] = EE_pi [sum^infinity_k=0 gamma^k R_k+t+1 | S_t = s]$

== 寻找最优策略的集中思路（选修）


