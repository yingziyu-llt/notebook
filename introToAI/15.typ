= MDP和动态规划

== MDP(马尔可夫决策过程)

=== 马尔可夫性
要求$S_t$只和$S_t-1$有关。代表了计算$V_pi$只需要看下一步就可以了，因为$V_pi,1=V_pi,2=...=V_pi,n$，于是往后任意相同步就只是下一步乘上一个常数。

=== 建模
1. 状态集合S
2. 动作集合A
3. 状态转移函数$P : <S,A,S> -> RR^+$,$P(s,a,s')=P[s'|s,a]$
4. 奖励函数$R:<S,A,RR^+> -> RR^+$

=== 状态价值和动作价值
$V_pi (S_1)=sum_a pi(a|s) sum_"s',r" p(s',r|s,a) [r + gamma V_pi(s')]$

$Q_pi (s,a) = sum_"s',r" p(s',r|s,a) [r + gamma V_pi(s')]$

于是得到了Bellman方程
$V_pi (s) = sum_a pi(a|s) Q_pi(s,a)$

定义$V^*(s) = max_pi V_pi (s)$叫做最优状态价值,最优策略就是能取到最优状态价值的策略$pi^*$

Bellman最优方程$V^*(s)=sum_{a} pi(a|s) Q^*(s,a)$

== 动态规划方法

适用于P,R已知的情况

== 策略迭代方法

=== 策略估值
对于任意策略$pi$,计算这个策略下的状态估值函数$V_pi$

使用Bellman方程的赋值形式：

$v_pi(s) = sum_a pi(a|s) sum_"s',r" p(s',r|s,a) [r + gamma v_pi(s')]$

迭代更新，$v_pi$是这个规则的不动点。

$v_"k+1" (s) = sum_a pi(a|s) sum_"s',r" p(s',r|s,a) [r + gamma v_k(s')]$

=== 策略提升
先有一个初始策略$pi_0$。
容易获得$pi_0$在$s_0$的期望收益$V_pi_0(s_0)$和动作价值$q_pi(s_0,a_1)$和$q_pi(s_0,a_2)$。

定义：在原基础上，根据原策略计算得到的值函数，贪心选择动作使得新策略的估值由于原策略。
需要注意的是，做了一次策略提升后，估值就不准了，要重新估值。

=== 策略迭代
交错进行策略提升和策略迭代，就可以收敛出最优的策略和值函数。

== 值迭代方法
只做一次估值，不需要等到估值收敛就直接开始策略提升。
=== 基本思想
定义一个状态的价值就是从这个状态出发所能达到的最优价值，计算所有状态的价值，贪心选择进行策略迭代。

== 广义策略迭代
每次更新部分的状态,使用算法优先更新热点区域。

== 采样分析
P,R都未知，通过采样方法逼近最优策略。

=== bootstrap
认为计算一个状态的估值是依据后继状态的估值

蒙特卡洛，时序差分自学。
